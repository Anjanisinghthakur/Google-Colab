{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9gb46CqsAM8mCHp2XkASH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anjanisinghthakur/Google-Colab/blob/main/Pyspark_DF3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ0jlU4qpAHm"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from pyspark.sql.functions import row_number, desc, rank, dense_rank, lead, lag, sum\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark=SparkSession.builder.appName(\"Myapp\").master(\"local[*]\").getOrCreate()\n",
        "employees = [\n",
        "    [\"John Doe\", 30, \"Software Engineer\", 60000],\n",
        "    [\"Jane Smith\", 28, \"Software Engineer\", 55000],\n",
        "    [\"Mike baby\", 35, \"Data Analyst\", 70000],\n",
        "    [\"Harry Potter\", 38, \"Data Analyst\", 55000],\n",
        "    [\"joe goe\", 25, \"Project Manager\", 65000],\n",
        "    [\"cue be\", 45, \"\", 75000],\n",
        "    [\"pagal\", 30, \"CEO\", 95000],\n",
        "    [\"lallu\", 50, \"CTO\", 105000]\n",
        "]\n",
        "\n",
        "employee_schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"Position\", StringType(), True),\n",
        "    StructField(\"Salary\", IntegerType(), True)\n",
        "])\n",
        "df=spark.createDataFrame(employees,schema=employee_schema)\n",
        "data = [\n",
        "    [\"USA\", \"Laptop\", 100],\n",
        "    [\"USA\", \"Mobile\", 200],\n",
        "    [\"India\", \"Laptop\", 150],\n",
        "    [\"India\", \"Mobile\", 300],\n",
        "    [\"India\", \"Tablet\", 50],\n",
        "]\n",
        "sch=\"Country string, Product string, Amount integer\"\n",
        "new_df=spark.createDataFrame(data, [\"country\", \"product\", \"amount\"])\n",
        "new_df.show()\n",
        "pivot_df=new_df.groupBy(\"product\").pivot(\"country\").sum(\"amount\")\n",
        "pivot_df.show()\n",
        "window=Window.orderBy((df.Salary))\n",
        "df1=df.withColumn(\"rn\",lead(df.Salary).over(window))\n",
        "df1.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oe8_mHN2pKNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from pyspark.sql.functions import row_number, desc, rank, dense_rank, lead, lag, sum, max,avg, count\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "spark=SparkSession.builder.appName(\"Myapp\").master(\"local[*]\").getOrCreate()\n",
        "data = [\n",
        "    [\"Alice\", \"HR\", 5000],\n",
        "    [\"Bob\", \"HR\", 4500],\n",
        "    [\"Charlie\", \"IT\", 7000],\n",
        "    [\"David\", \"IT\", 6000],\n",
        "    [\"Eve\", \"Finance\", 6500],\n",
        "    [\"Frank\", \"Finance\", 6200],\n",
        "    [\"Grace\", \"Marketing\", 4800],\n",
        "    [\"Heidi\", \"Marketing\", 5100],\n",
        "]\n",
        "schema_str=\"Name string, dept string, salary integer\"\n",
        "df=spark.createDataFrame(data, schema=schema_str)\n",
        "window_spec=Window.partitionBy(df.dept)\n",
        "res=df.withColumn(\"aveg\",count(df.Name).over(window_spec))\n",
        "res.show()\n",
        "res=res.select(res.Name, res.dept,res.salary).where(res.salary>res.aveg)\n",
        "res.show()"
      ],
      "metadata": {
        "id": "5k-zGpAKQXlU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}