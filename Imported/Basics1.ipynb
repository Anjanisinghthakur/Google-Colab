from pyspark.sql import SparkSession

spark = SparkSession.builder \
        .master('local[*]') \
        .appName('Basics') \
        .getOrCreate()
lst=[100,2,5,6,8,10,1]
rr= spark.sparkContext.parallelize(lst,2)
rr.reduce(lambda x,y: x+y)
rr.reduce(lambda x,y: x*y) 
#join dataframes
df1=spark.read.csv('/content/sample_data/file1.csv', header=True)
df1.show()
df2=spark.read.csv('/content/sample_data/file2.csv', header=True)
df2.show()
res=df1.join(df2,on='empID',how='right')
res.show()
print(f'Data types of all the columns is : {res.dtypes}')
#Type conversion
res1=df = res.withColumn("Salary", res["Salary"].cast("integer"))
#Aggregarion
df3=res1.groupBy('Position').sum('Salary')
df3.show()
#Filtering Data
df4=df3.filter(res['Position']=='DE')
df4.show()
